# Ð˜ÐœÐŸÐžÐ Ð¢ Ð”Ð›Ð¯ MODEL.FIT
import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
from tokenizers import Tokenizer
from tensorflow.keras.callbacks import ModelCheckpoint
import time
import gc
from tensorflow.keras.callbacks import LearningRateScheduler
import shutil
import  os
from collections import deque
import numpy as np
from functools import partial



# ÐŸÐÐ ÐÐœÐ¡ Ð”Ð›Ð¯ MODEL.FIT

tf.config.optimizer.set_jit(True)

my_tokenizer_path = 'bpe_tokenizer_15k268files.json'
tokenizer = Tokenizer.from_file(my_tokenizer_path)
vocab_size = tokenizer.get_vocab_size()

def load_tokens(token_file_path):
    tokens = np.load(token_file_path)
    return tokens
#tokens = load_tokens('tokens_15kÐ¿Ð¾Ð´Ñ€.npy')
tokens = load_tokens('tokens_15k268.npy')
print(f"tokens count = {len(tokens)}")

model_scale = 4 # Ð¼Ð°ÑÑˆÑ‚Ð°Ð± Ð±Ð»Ð¾ÐºÐ° Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼ÐµÑ€Ð°
num_transformer_blocks = 6 # ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð±Ð»Ð¾ÐºÐ¾Ð² Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼ÐµÑ€Ð°
# create_nn = True
# nn_file = 'output/sl16_b250_as1/model_checkpoint_3.35.keras'
use_lr_scheduler = False
initial_lr = 0.001 #1e-7 #0.001

sequence_length = 128 # Ñ€Ð°Ð·Ð¼ÐµÑ€ ÑÑÐ¼Ð¿Ð»Ð°
# Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð±Ð°Ñ‚Ñ‡Ð° Ð±Ð°Ñ‚Ñ‡Ð° Ð´Ð»Ñ Ñ‚Ð¿Ñƒ ÐºÑ€Ð°Ñ‚128
accumulation_steps = 10  # ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑˆÐ°Ð³Ð¾Ð² Ð´Ð»Ñ Ð½Ð°ÐºÐ¾Ð¿Ð»ÐµÐ½Ð¸Ñ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð¾Ð²
batch_size = int(100000/sequence_length/accumulation_steps) #512 #8*50 #int(150000/sequence_length)
effective_batch_size = batch_size * accumulation_steps
state_file = 'training_state.npy'

steps_per_epoch = 200 #1000 #15 #for lr_scheduler #500 #token_count // (batch_size * sequence_length)
epochs = len(tokens) // (steps_per_epoch * batch_size * accumulation_steps) #5
save_freq='epoch'

if os.path.exists(state_file):
    create_nn = False
    state = np.load(state_file, allow_pickle=True).item()
    start_step = state.get('step', 0)
    saved_step_index = state.get('feistel_index', 0)
    start_epoch = int(saved_step_index / (steps_per_epoch * batch_size * accumulation_steps))  #state.get('epoch', 0)
    nn_file = state.get('nn_file', 0)
    print(f"ðŸ” Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ: ÑÐ¿Ð¾Ñ…Ð° {start_epoch}, ÑˆÐ°Ð³ {start_step}, Ð¸Ð½Ð´ÐµÐºÑ {saved_step_index}, Ð¼Ð¾Ð´ÐµÐ»ÑŒ {nn_file}")
else:
    create_nn = True
    start_epoch = 0
    start_step = 0
    saved_step_index = 0



# ÐžÐ‘Ð£Ð§Ð•ÐÐ˜Ð• Ð¡Ð•Ð¢Ð˜ GRAD_ACCUM ÐœÐžÐ” DEEPSEEK

# RAM
# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð±Ð°Ñ‚Ñ‡ÐµÐ¹ Ð¸Ð· Ð¼Ð°ÑÑÐ¸Ð²Ð° Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²
'''def generate_batch(tokens, batch_size, sequence_length):
    indices = np.random.randint(0, len(tokens) - sequence_length - 1, batch_size)
    X = np.zeros((batch_size, sequence_length), dtype=np.int64)
    Y = np.zeros((batch_size, sequence_length), dtype=np.int64)

    for i, idx in enumerate(indices):
        X[i] = tokens[idx:idx + sequence_length]
        Y[i] = tokens[idx + 1:idx + 1 + sequence_length]

    return X, Y'''
    
'''def feistel_shuffle_index(i, n, rounds=3, key=0xA5A5A5A5):
    """Ð”ÐµÑ‚ÐµÑ€Ð¼Ð¸Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð¿ÑÐµÐ²Ð´Ð¾ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ð¹ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº, Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÑŽÑ‰Ð¸Ð¹ Ð¸Ñ‚ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒÑÑ Ð±ÐµÐ· Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð¾Ð²."""
    l = i & 0xFFFF
    r = i >> 16
    for _ in range(rounds):
        l, r = r, l ^ ((hash((r, key)) & 0xFFFFFFFF) % n)
    return ((r << 16) | l) % n'''

def lcg_shuffle(i, n, a=50000009, c=0, m=None):
    if m is None:
        m = 1
        while m < n:
            m <<= 1
    x = i
    while True:
        x = (a * x + c) % m
        if x < n:
            return x

'''def data_generator():
    while True:
        yield generate_batch(tokens, batch_size, sequence_length)'''

def generator(start_index):
    step = start_index
    while True:
        #idx = feistel_shuffle_index(step, len(tokens) - sequence_length - 1, key=0xA5A5A5A5)
        idx = lcg_shuffle(step, len(tokens) - sequence_length - 1)
        x = tokens[idx : idx + sequence_length]
        y = tokens[idx + 1 : idx + 1 + sequence_length]
        yield x, y
        step += 1

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¸ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ TPU
try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    tf.config.experimental_connect_to_cluster(tpu)
    if 'strategy' not in globals():
        tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.TPUStrategy(tpu)
    print('Running on TPU')
    print("All devices: ", tf.config.list_logical_devices('TPU'))
except ValueError:
    strategy = tf.distribute.get_strategy()
    print('Running on default strategy (CPU/GPU)')
    print("All devices CPU: ", tf.config.list_logical_devices('CPU'))
    print("All devices GPU: ", tf.config.list_logical_devices('GPU'))

class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, trainable=True, dtype='float32', rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential(
            [layers.Dense(ff_dim, activation="relu"),
             layers.Dense(embed_dim),]
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)

    def build(self, input_shape):
            # Ð—Ð´ÐµÑÑŒ Ð¼Ð¾Ð¶Ð½Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð»ÑŽÐ±ÑƒÑŽ Ð»Ð¾Ð³Ð¸ÐºÑƒ Ð´Ð»Ñ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð²ÐµÑÐ¾Ð²
            super(TransformerBlock, self).build(input_shape)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)
tf.keras.utils.get_custom_objects().update({'TransformerBlock': TransformerBlock})

print(f"Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ - {create_nn}")
if create_nn:
    with strategy.scope():
        # Ð·Ð´ÐµÑÑŒ Ð±Ñ‹Ð» ÐºÐ»Ð°ÑÑ TransformerBlock
        embed_dim = 16 * model_scale  # Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð²Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ
        num_heads = 1 * model_scale  # ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð³Ð¾Ð»Ð¾Ð² Ð² Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼Ðµ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ
        ff_dim = 16 * model_scale * 4  # Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ð¾Ð»Ð½Ð¾ÑÐ²ÑÐ·Ð½Ð¾Ð³Ð¾ ÑÐ»Ð¾Ñ

        inputs = tf.keras.Input(shape=(None,))
        embedding_layer = layers.Embedding(
            input_dim=vocab_size,
            output_dim=embed_dim
        )
        x = embedding_layer(inputs)
        for _ in range(num_transformer_blocks):
            transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)
            x = transformer_block(x, training=True)
        outputs = layers.Dense(vocab_size, activation='softmax')(x)

        model = tf.keras.Model(inputs=inputs, outputs=outputs)
        model.compile(optimizer='adam',
                      loss='sparse_categorical_crossentropy')
else:
    with strategy.scope():
        print("Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸")
        model = tf.keras.models.load_model(nn_file)

    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy')

checkpoint_dir = f'output/sl{sequence_length}_b{batch_size}_as{accumulation_steps}'
os.makedirs(checkpoint_dir, exist_ok=True)

checkpoint_filepath = os.path.join(checkpoint_dir, 'model_checkpoint_{accuracy:.2f}.keras')

checkpoint_callback = ModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='loss',
    verbose=1,
    save_best_only=True,
    save_freq=save_freq,
    #save_format='tf'
)
print(f'save_freq = {int(50000/sequence_length)}')

print("ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸")

start_time = time.time()

'''dataset = tf.data.Dataset.from_generator(
    data_generator,
    output_signature=(
        tf.TensorSpec(shape=(batch_size, sequence_length), dtype=tf.int64),
        tf.TensorSpec(shape=(batch_size, sequence_length), dtype=tf.int64),
    )
)
#dataset = dataset.batch(128, drop_remainder=True) # Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ Ñ„Ð¸ÐºÑ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°, Ñ‚.Ðµ. Ñ Ð½Ð¸Ð¼ ÐºÐ¾Ð´ Ð½Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚
dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)'''
dataset = tf.data.Dataset.from_generator(
    partial(generator, start_index=saved_step_index),
    output_signature=(
        tf.TensorSpec(shape=(sequence_length,), dtype=tf.int64),
        tf.TensorSpec(shape=(sequence_length,), dtype=tf.int64),
    )
).batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)

@tf.function
def apply_accumulated_gradients(accumulated_gradients):
    def apply_fn(gradients):
        non_none_gradients = [(acc_g, var) for acc_g, var in zip(gradients, model.trainable_variables) if acc_g is not None]
        non_none_gradients = [(tf.reduce_sum(acc_g, axis=None), var) for acc_g, var in non_none_gradients]
        model.optimizer.apply_gradients(non_none_gradients)
        return [tf.zeros_like(g) if g is not None else None for g in gradients]

    accumulated_gradients = strategy.run(apply_fn, args=(accumulated_gradients,))
    return accumulated_gradients

# Ð˜ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ñ‹Ð¹ GradientAccumulator Ð´Ð»Ñ TPU
class GradientAccumulator:
    def __init__(self):
        self._gradients = []
        self._accumulation_steps = tf.Variable(initial_value=0, dtype=tf.int32, trainable=False)

    @property
    def gradients(self):
        return [g.read_value() for g in self._gradients]

    def initialize(self, model, accumulation_steps):
        self._accumulation_steps.assign(accumulation_steps)
        if not self._gradients:
            # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð²Ð½Ðµ tf.function
            with strategy.scope():
                self._gradients = [
                    tf.Variable(
                        initial_value=tf.zeros_like(var),
                        dtype=var.dtype,
                        trainable=False,
                        synchronization=tf.VariableSynchronization.ON_READ
                    ) for var in model.trainable_variables
                ]

    def reset(self):
        for g in self._gradients:
            g.assign(tf.zeros_like(g))

accumulator = GradientAccumulator()

# Ð£Ð¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ñ‹Ð¹ train_step
@tf.function
def train_step(iterator, accumulator):
    def step_fn(inputs, targets):
        with tf.GradientTape() as tape:
            predictions = model(inputs, training=True)
            loss = model.compiled_loss(targets, predictions)

        gradients = tape.gradient(loss, model.trainable_variables)
        #gradients = [tf.convert_to_tensor(g) if isinstance(g, tf.IndexedSlices) else g for g in gradients]
        gradients = [g / tf.cast(accumulator._accumulation_steps, g.dtype) for g in gradients]

        for i in range(len(gradients)):
            if gradients[i] is not None: # NEW
                accumulator._gradients[i].assign_add(gradients[i])

        return loss

    inputs, targets = next(iterator)
    per_replica_loss = strategy.run(step_fn, args=(inputs, targets))
    return strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_loss, axis=None)

# Ð£Ð¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ñ‹Ð¹ apply_gradients
@tf.function
def apply_gradients(accumulator):
    def apply_fn():
        model.optimizer.apply_gradients(zip(accumulator.gradients, model.trainable_variables))
        for g in accumulator._gradients:
            g.assign(tf.zeros_like(g))

    strategy.run(apply_fn)

print(f"model_scale = {model_scale}, num_transformer_blocks = {num_transformer_blocks}")
print(f"sequence = {sequence_length}, batch = {batch_size}, accum_steps = {accumulation_steps}")
step_times = deque(maxlen=100)  # Ñ…Ñ€Ð°Ð½Ð¸Ð¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 100 ÑˆÐ°Ð³Ð¾Ð²
start_time = time.time()
iterator = iter(dataset)

# Ð˜ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ñ‹Ð¹ Ñ†Ð¸ÐºÐ» Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ
#for epoch in range(epochs):
for epoch in range(start_epoch, epochs):
    print(f"\nEpoch {epoch + 1}/{epochs}")
    #iterator = iter(dataset)

    # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð¾Ð»Ð¶Ð½Ð° Ð±Ñ‹Ñ‚ÑŒ Ð²Ð½Ðµ Ñ†Ð¸ÐºÐ»Ð° ÑˆÐ°Ð³Ð¾Ð²
    accumulator.initialize(model, accumulation_steps)

    #for step in range(steps_per_epoch):
    step_range = range(start_step, steps_per_epoch) if epoch == start_epoch else range(steps_per_epoch)

    for step in step_range:
        step_start = time.time()
        
        # ÐÐ°ÐºÐ¾Ð¿Ð»ÐµÐ½Ð¸Ðµ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð¾Ð²
        for _ in range(accumulation_steps):
            loss = train_step(iterator, accumulator)

        # ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð²ÐµÑÐ¾Ð²
        apply_gradients(accumulator)

        # Ð Ð°ÑÑ‡Ñ‘Ñ‚ ETA, ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ trimmed mean (ÑƒÐ´Ð°Ð»ÑÐµÐ¼ Ð²ÐµÑ€Ñ…Ð½Ð¸Ðµ Ð¸ Ð½Ð¸Ð¶Ð½Ð¸Ðµ 10%)
        step_times.append(time.time() - step_start)
        t = np.sort(step_times)
        t = t[len(t)//10 : -len(t)//10 or None]
        avg = t.mean() if len(t) else np.array(step_times).mean()
        s = epoch * steps_per_epoch + step + 1
        r = epochs * steps_per_epoch - s
        eta_m, eta_s = divmod(int(avg * r), 60)
        
        # Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
        print(f"\rStep {step+1}/{steps_per_epoch} Loss: {loss:.4f} | ETA: {eta_m:02d}:{eta_s:02d}", end='',flush=True)
    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ ÐºÐ°Ð¶Ð´ÑƒÑŽ ÑÐ¿Ð¾Ñ…Ñƒ
    model.save(checkpoint_filepath.format(accuracy=loss.numpy()))
    
    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð¿Ð¾ÑÐ»Ðµ ÑÐ¿Ð¾Ñ…Ð¸
    state = {
        'epoch': epoch + 1,
        'step': 0,
        'feistel_index': (epoch + 1) * steps_per_epoch * batch_size * accumulation_steps,
        'nn_file': checkpoint_filepath.format(accuracy=loss.numpy())
    }
    np.save(state_file, state)

if "iterator" in globals():
    del iterator
    gc.collect()
elapsed_time = time.time() - start_time
minutes = int(elapsed_time // 60)
seconds = int(elapsed_time % 60)
print(f"\nTotal elapsed time: {minutes}:{seconds}")